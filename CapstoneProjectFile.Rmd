---
title: "CapstoneProject"
author: "Kevin Goldsmith"
date: "10/24/2019"
output: html_document
---

```{r milestone intro, echo = FALSE, results = 'asis', message = FALSE, warning = FALSE, error = FALSE}
cat('The objective of this report is to provide an update on the Data Science Capstone project. So far, I have created algorithms to read in data from each of the three text databases, done some exploratory analysis, and created a simple text prediction model.

The three databases are extremely large, so providing only approximated file sizes in the interest of computation time:')
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load libraries, message = FALSE, echo = FALSE}
set.seed(12345)

library(readr)
library(data.table)
library(tm)
library(quanteda)
library(readtext)
library(methods)
library(utils)
library(LaF)
library(ggplot2)
```

```{r connect to database, echo = FALSE}
myURL <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
 
dateDownloaded <- date()
 
 if(!file.exists("rawdata.zip")){
     download.file(myURL, destfile = "rawdata.zip", method = "curl")
     unzip("rawdata.zip")
}
```

```{r function to read in data, cache= TRUE, echo = FALSE, results = 'asis', message = FALSE, warning = FALSE, error = FALSE}

create_file <- function(file_name, URL, records = 1000, increment = 10000) {

  assign(deparse(substitute(file_name)), rep(0, records), envir = .GlobalEnv)
  helper <- rep(0, records)
    
  flag = 0
  n_lines <<- 0
  while (flag != 1){
    a = length(readr::read_lines(URL, skip = n_lines, n_max = 2))
    if(a == 0) {flag = 1}
    n_lines <<- n_lines + increment
  }
  n_lines <<- n_lines - increment * 2
  flag = 0
  cat(deparse(substitute(file_name)), "has approximately", n_lines, "lines",
      sep = " ")
  for (i in 1:records){
    helper[i] <- readr::read_lines(URL, skip =
                              round(runif(1, min = 1, max = n_lines)),
                              n_max = 1)
  }

  assign(deparse(substitute(file_name)), helper, envir = .GlobalEnv)
}
```

```{r read in profanity, echo = FALSE}
download.file("https://raw.githubusercontent.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/master/en", 
              destfile = "profanity.txt", method = "curl")

profanity <- readLines("profanity.txt")


```

```{r read in all English words, echo = FALSE}
download.file("https://raw.githubusercontent.com/dwyl/english-words/master/words.txt", 
              destfile = "english.txt", method = "curl")

all_english <- readLines("english.txt")
```


```{r read in US files, create combined corpus and DFM, echo = FALSE}
create_file(US.blogs_corp, "~/CapstoneProject/final/en_US/en_US.blogs.txt", 100)
create_file(US.twit_corp, "~/CapstoneProject/final/en_US/en_US.twitter.txt", 100)
create_file(US.news_corp, "~/CapstoneProject/final/en_US/en_US.news.txt", 100)

US.blogs_corp <- corpus(US.blogs_corp)
US.news_corp <- corpus(US.news_corp)
US.twit_corp <- corpus(US.twit_corp)
combined_corp <- US.blogs_corp + US.news_corp + US.twit_corp
```

 
```{r tokenize and DFM, remove unneeded, echo = FALSE}
combined_tok <- tokens(combined_corp)
combined_tok <- tokens(combined_tok, remove_punct = TRUE, remove_numbers = TRUE)
combined_tok <- tokens_select(combined_tok, pattern = profanity, selection = "remove")
combined_tok <- tokens_select(combined_tok, pattern = all_english, selection = "keep")
combined_dfm <- dfm(combined_tok)
combined_ngrams <- tokens_ngrams(combined_tok, 2:3)
rm(profanity)
rm(all_english)
```

 
```{r quiz 1 functions, eval = FALSE, echo = FALSE}
max(sapply(US.blogs_corp,nchar))
max(sapply(US.news_corp,nchar))
max(sapply(US.twit_corp,nchar))

textstat_frequency(dfm_select(dfm(US.twit_corp), pattern = "love", case_insensitive =  FALSE))$docfreq/
  textstat_frequency(dfm_select(dfm(US.twit_corp), pattern = "hate", case_insensitive = FALSE))$docfreq

textstat_frequency(dfm_select(dfm(US.twit_corp), pattern = "biostats"))

textstat_frequency(dfm_select(dfm(US.twit_corp), pattern = 
                                "A computer once beat me at chess, but it was no match for me at kickboxing")
                   )

```


```{r show top frequency, eval = FALSE, echo = FALSE}
textplot_wordcloud(combined_dfm)

```


```{r how many words needed for x% coverage?, eval = FALSE, echo = FALSE}
coverage <- function(dfm, percent){
  flag = 0
  partial = 0
  total = sum(textstat_frequency(dfm)$frequency)
  i = 1
  
  while (flag == 0){
    partial = partial + textstat_frequency(dfm)[i]$frequency
    if (partial >= total * (percent / 100)) {flag = 1}
    i = i + 1
  }
  i
}

coverage(combined_dfm, 50)
coverage(combined_dfm, 90)
```

 
```{r next word predictor function, echo = FALSE}
next_word <- function(firstword){
  possible_words <- kwic(combined_ngrams,
                         pattern = paste0("^",deparse(substitute(firstword)),"+_[^_]+$"),
                         valuetype = "regex")
  starts <- unname(sapply(possible_words[,5], str_locate, "_")[2,]) + 1
  ends <- unname(sapply(possible_words[,5], nchar))
  possible_words <- substr(possible_words[,5], starts, ends)
  selections <- as.data.frame(table(possible_words))
  selections$likelihood<- prop.table(selections$Freq)
  selections <- subset(selections, select = c(1,3))
  selections <- selections[order(-selections$likelihood),]
  rownames(selections) <- NULL
  head(selections, 10)
}
```

```{r milestone report, echo = FALSE, results = 'asis', message = FALSE, warning = FALSE, error = FALSE}
cat(length(texts(US.blogs_corp)), 'lines were randomly sampled from each of the three databases to work with for initial analysis. More data will be used in building the final model. These',
    length(texts(US.blogs_corp)) *3, 
    'lines were then combined into a single corpus containing',
    sum(textstat_frequency(dfm(US.blogs_corp))$frequency), 
    'total words. Profane and non-English words were removed from this corpus. Here is a summary of the top 25 most frequent words in the corpus, and then number of times they appear:', 
    sep = " ")
```

```{r, echo = FALSE, message = FALSE}
textstat_frequency(combined_dfm, n = 25)[,1:2]
print('Distribution of words by number of times they appear')
temp <- textstat_frequency(combined_dfm)$frequency
temp <- ifelse(temp > 20, 20, temp)
qplot(temp, geom = "histogram", xlim = c(0,20))
```